{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - facebookresearch\n",
      " - defaults\n",
      " - conda-forge\n",
      " - anaconda\n",
      " - pytorch\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): failed\n",
      "\n",
      "UnavailableInvalidChannel: HTTP 404 NOT FOUND for channel facebookresearch <https://conda.anaconda.org/facebookresearch>\n",
      "\n",
      "The channel is not accessible or is invalid.\n",
      "\n",
      "You will need to adjust your conda configuration to proceed.\n",
      "Use `conda config --show channels` to view your configuration's current state,\n",
      "and use `conda config --show-sources` to view config file locations.\n",
      "\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} pip install fvcore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Analysis Results:\n",
      "Total files: 3291\n",
      "Valid files: 2523 (76.7%)\n",
      "Class distribution: [1186, 1337] (Real: 1186, Fake: 1337)\n",
      "Class ratio: 1.13:1\n",
      "Sample rates found: {44100}\n",
      "Duration statistics (seconds):\n",
      "  min: 2.00\n",
      "  max: 1569.00\n",
      "  mean: 7.84\n",
      "  median: 5.00\n",
      "  std: 48.78\n",
      "\n",
      "Running experiment: S3D_Pretrained\n",
      "Using cpu\n",
      "Class weights: tensor([1.0637, 0.9435])\n",
      "\n",
      "=== Training Fold 1 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/azureuser/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fvcore'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 714\u001b[0m\n\u001b[1;32m    712\u001b[0m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoint_dir\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbase_checkpoint_dir\u001b[39m\u001b[38;5;124m'\u001b[39m], exp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mRunning experiment: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 714\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 481\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Training Fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    480\u001b[0m \u001b[38;5;66;03m# Initialize model\u001b[39;00m\n\u001b[0;32m--> 481\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    483\u001b[0m \u001b[38;5;66;03m# Optimizer and criterion\u001b[39;00m\n\u001b[1;32m    484\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m    485\u001b[0m                         weight_decay\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0.01\u001b[39m))\n",
      "Cell \u001b[0;32mIn[1], line 286\u001b[0m, in \u001b[0;36mS3D.__init__\u001b[0;34m(self, num_classes)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m--> 286\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ms3d \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfacebookresearch/pytorchvideo\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms3d\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;66;03m# Modify for audio input (assuming waveform input)\u001b[39;00m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ms3d\u001b[38;5;241m.\u001b[39mblocks[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mconv \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mConv3d(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m64\u001b[39m, kernel_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m7\u001b[39m), stride\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m), padding\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n",
      "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.10/site-packages/torch/hub.py:558\u001b[0m, in \u001b[0;36mload\u001b[0;34m(repo_or_dir, model, source, trust_repo, force_reload, verbose, skip_validation, *args, **kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgithub\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    555\u001b[0m     repo_or_dir \u001b[38;5;241m=\u001b[39m _get_cache_or_reload(repo_or_dir, force_reload, trust_repo, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    556\u001b[0m                                        verbose\u001b[38;5;241m=\u001b[39mverbose, skip_validation\u001b[38;5;241m=\u001b[39mskip_validation)\n\u001b[0;32m--> 558\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43m_load_local\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_or_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.10/site-packages/torch/hub.py:584\u001b[0m, in \u001b[0;36m_load_local\u001b[0;34m(hubconf_dir, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _add_to_sys_path(hubconf_dir):\n\u001b[1;32m    583\u001b[0m     hubconf_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(hubconf_dir, MODULE_HUBCONF)\n\u001b[0;32m--> 584\u001b[0m     hub_module \u001b[38;5;241m=\u001b[39m \u001b[43m_import_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODULE_HUBCONF\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhubconf_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    586\u001b[0m     entry \u001b[38;5;241m=\u001b[39m _load_entry_from_hubconf(hub_module, model)\n\u001b[1;32m    587\u001b[0m     model \u001b[38;5;241m=\u001b[39m entry(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.10/site-packages/torch/hub.py:98\u001b[0m, in \u001b[0;36m_import_module\u001b[0;34m(name, path)\u001b[0m\n\u001b[1;32m     96\u001b[0m module \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mmodule_from_spec(spec)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(spec\u001b[38;5;241m.\u001b[39mloader, Loader)\n\u001b[0;32m---> 98\u001b[0m \u001b[43mspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexec_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/.cache/torch/hub/facebookresearch_pytorchvideo_main/hubconf.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m dependencies \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorchvideo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401, E402\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     c2d_r50,\n\u001b[1;32m      6\u001b[0m     csn_r101,\n\u001b[1;32m      7\u001b[0m     efficient_x3d_s,\n\u001b[1;32m      8\u001b[0m     efficient_x3d_xs,\n\u001b[1;32m      9\u001b[0m     i3d_r50,\n\u001b[1;32m     10\u001b[0m     mvit_base_16,\n\u001b[1;32m     11\u001b[0m     mvit_base_16x4,\n\u001b[1;32m     12\u001b[0m     mvit_base_32x3,\n\u001b[1;32m     13\u001b[0m     r2plus1d_r50,\n\u001b[1;32m     14\u001b[0m     slow_r50,\n\u001b[1;32m     15\u001b[0m     slow_r50_detection,\n\u001b[1;32m     16\u001b[0m     slowfast_16x8_r101_50_50,\n\u001b[1;32m     17\u001b[0m     slowfast_r101,\n\u001b[1;32m     18\u001b[0m     slowfast_r50,\n\u001b[1;32m     19\u001b[0m     slowfast_r50_detection,\n\u001b[1;32m     20\u001b[0m     x3d_l,\n\u001b[1;32m     21\u001b[0m     x3d_m,\n\u001b[1;32m     22\u001b[0m     x3d_s,\n\u001b[1;32m     23\u001b[0m     x3d_xs,\n\u001b[1;32m     24\u001b[0m )\n",
      "File \u001b[0;32m~/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/models/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcsn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_csn\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhead\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_res_basic_head, ResNetBasicHead\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmasked_multistream\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      6\u001b[0m     LearnMaskedDefault,\n\u001b[1;32m      7\u001b[0m     LSTM,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     TransposeTransformerEncoder,\n\u001b[1;32m     13\u001b[0m )\n",
      "File \u001b[0;32m~/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/models/csn.py:8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorchvideo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhead\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_res_basic_head\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorchvideo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresnet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_bottleneck_block, create_res_stage, Net\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorchvideo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_res_basic_stem\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_csn\u001b[39m(\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Input clip configs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m     head_output_with_global_average: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     45\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m nn\u001b[38;5;241m.\u001b[39mModule:\n",
      "File \u001b[0;32m~/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/models/resnet.py:10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorchvideo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m set_attributes\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorchvideo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhead\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_res_basic_head, create_res_roi_pooling_head\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorchvideo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DetectionBBoxNetwork, Net\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorchvideo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     12\u001b[0m     create_acoustic_res_basic_stem,\n\u001b[1;32m     13\u001b[0m     create_res_basic_stem,\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_bottleneck_block\u001b[39m(\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# Convolution configs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m     activation: Callable \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mReLU,\n\u001b[1;32m     40\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m nn\u001b[38;5;241m.\u001b[39mModule:\n",
      "File \u001b[0;32m~/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/models/net.py:8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorchvideo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m set_attributes\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorchvideo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mweight_init\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init_net_weights\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mNet\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m     12\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m    Build a general Net models with a list of blocks for video recognition.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m    The ResNet builder can be found in `create_resnet`.\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/models/weight_init.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfvcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mweight_init\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m c2_msra_fill, c2_xavier_fill\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorchvideo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SpatioTemporalClsPositionalEncoding\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_init_resnet_weights\u001b[39m(model: nn\u001b[38;5;241m.\u001b[39mModule, fc_init_std: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fvcore'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import glob\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, roc_auc_score, roc_curve\n",
    "import torchaudio.transforms as T\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, CyclicLR\n",
    "from torch.optim.swa_utils import AveragedModel, SWALR\n",
    "import librosa  # Added import\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Dataset Analysis Function\n",
    "def analyze_dataset(protocols_dir, audio_dir):\n",
    "    \"\"\"Analyze dataset characteristics including class balance and audio durations\"\"\"\n",
    "    train_files = glob.glob(f\"{protocols_dir}/train_fold*.csv\")\n",
    "    \n",
    "    class_counts = [0, 0]\n",
    "    durations = []\n",
    "    sample_rates = set()\n",
    "    valid_files = 0\n",
    "    total_files = 0\n",
    "    \n",
    "    for fold_file in train_files:\n",
    "        annos = pd.read_csv(fold_file)\n",
    "        for idx in range(len(annos)):\n",
    "            total_files += 1\n",
    "            clip_name = annos.iloc[idx, 0]\n",
    "            audio_path = os.path.join(audio_dir, f\"{clip_name}.wav\")\n",
    "            \n",
    "            if os.path.exists(audio_path):\n",
    "                try:\n",
    "                    metadata = torchaudio.info(audio_path)\n",
    "                    duration = metadata.num_frames / metadata.sample_rate\n",
    "                    durations.append(duration)\n",
    "                    sample_rates.add(metadata.sample_rate)\n",
    "                    \n",
    "                    label = 0 if any(k in str(annos.iloc[idx, 1]).lower() for k in ['truth', '0']) else 1\n",
    "                    class_counts[label] += 1\n",
    "                    valid_files += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Error analyzing {audio_path}: {e}\")\n",
    "    \n",
    "    duration_stats = {\n",
    "        'min': min(durations),\n",
    "        'max': max(durations),\n",
    "        'mean': np.mean(durations),\n",
    "        'median': np.median(durations),\n",
    "        'std': np.std(durations)\n",
    "    }\n",
    "    \n",
    "    print(\"\\nDataset Analysis Results:\")\n",
    "    print(f\"Total files: {total_files}\")\n",
    "    print(f\"Valid files: {valid_files} ({valid_files/total_files:.1%})\")\n",
    "    print(f\"Class distribution: {class_counts} (Real: {class_counts[0]}, Fake: {class_counts[1]})\")\n",
    "    print(f\"Class ratio: {class_counts[1]/class_counts[0]:.2f}:1\")\n",
    "    print(f\"Sample rates found: {sample_rates}\")\n",
    "    print(\"Duration statistics (seconds):\")\n",
    "    for k, v in duration_stats.items():\n",
    "        print(f\"  {k}: {v:.2f}\")\n",
    "    \n",
    "    return {\n",
    "        'class_counts': class_counts,\n",
    "        'duration_stats': duration_stats,\n",
    "        'sample_rates': sample_rates,\n",
    "        'valid_ratio': valid_files/total_files\n",
    "    }\n",
    "\n",
    "# Enhanced AudioDataset Class\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, annotations_file, audio_dir, target_length=16000, mode='spectrogram', \n",
    "                 transform=None, spec_transform=None, spec_params=None):\n",
    "        self.annos = pd.read_csv(annotations_file)\n",
    "        self.audio_dir = audio_dir\n",
    "        self.target_length = target_length\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "        self.spec_transform = spec_transform\n",
    "        self.spec_params = spec_params or {\n",
    "            'n_mels': 128,\n",
    "            'n_fft': 2048,\n",
    "            'hop_length': 512,\n",
    "            'f_min': 20,\n",
    "            'f_max': 8000\n",
    "        }\n",
    "        \n",
    "        self.valid_data = []\n",
    "        for idx in range(len(self.annos)):\n",
    "            clip_name = self.annos.iloc[idx, 0]\n",
    "            audio_path = os.path.join(self.audio_dir, f\"{clip_name}.wav\")\n",
    "            if os.path.exists(audio_path):\n",
    "                label = self._process_label(self.annos.iloc[idx, 1])\n",
    "                self.valid_data.append((audio_path, label))\n",
    "        \n",
    "        print(f\"Found {len(self.valid_data)}/{len(self.annos)} valid files.\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.valid_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        audio_path, label = self.valid_data[idx]\n",
    "        try:\n",
    "            waveform, sample_rate = torchaudio.load(audio_path)\n",
    "            waveform = waveform.mean(dim=0)  # Mono\n",
    "            waveform = torchaudio.functional.resample(waveform, sample_rate, 16000)\n",
    "            \n",
    "            if waveform.shape[0] > self.target_length:\n",
    "                start = torch.randint(0, waveform.shape[0] - self.target_length, (1,)).item()\n",
    "                waveform = waveform[start:start + self.target_length]\n",
    "            else:\n",
    "                padding = self.target_length - waveform.shape[0]\n",
    "                waveform = torch.nn.functional.pad(waveform, (0, padding))\n",
    "            \n",
    "            if self.transform:\n",
    "                waveform = self.transform(waveform)\n",
    "            \n",
    "            if self.mode == 'waveform':\n",
    "                data = waveform\n",
    "            elif self.mode == 'spectrogram':\n",
    "                spectrogram = self._create_spectrogram(waveform)\n",
    "                if self.spec_transform:\n",
    "                    spectrogram = self.spec_transform(spectrogram)\n",
    "                data = spectrogram\n",
    "            elif self.mode == 'both':\n",
    "                spectrogram = self._create_spectrogram(waveform)\n",
    "                if self.spec_transform:\n",
    "                    spectrogram = self.spec_transform(spectrogram)\n",
    "                data = (waveform, spectrogram)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid mode\")\n",
    "            \n",
    "            return data, torch.tensor(label)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Error loading {audio_path}: {e}\")\n",
    "            return None, None\n",
    "    \n",
    "    def _create_spectrogram(self, waveform):\n",
    "        mel_spec = T.MelSpectrogram(\n",
    "            sample_rate=16000,\n",
    "            n_mels=self.spec_params['n_mels'],\n",
    "            n_fft=self.spec_params['n_fft'],\n",
    "            hop_length=self.spec_params['hop_length'],\n",
    "            f_min=self.spec_params['f_min'],\n",
    "            f_max=self.spec_params['f_max']\n",
    "        )(waveform)\n",
    "        db_spec = T.AmplitudeToDB()(mel_spec)\n",
    "        return db_spec.unsqueeze(0)\n",
    "    \n",
    "    def _process_label(self, label_str):\n",
    "        str_label = str(label_str).strip().lower()\n",
    "        return 0 if any(k in str_label for k in ['truth', '0']) else 1\n",
    "\n",
    "# Collate Function\n",
    "def audio_collate_fn(batch):\n",
    "    batch = [item for item in batch if item[0] is not None]\n",
    "    if not batch:\n",
    "        return torch.zeros(1, 16000), torch.tensor([0])\n",
    "    inputs, labels = zip(*batch)\n",
    "    if isinstance(inputs[0], tuple):\n",
    "        waveforms, spectrograms = zip(*inputs)\n",
    "        waveforms = torch.stack(waveforms)\n",
    "        spectrograms = torch.stack(spectrograms)\n",
    "        inputs = (waveforms, spectrograms)\n",
    "    else:\n",
    "        inputs = torch.stack(inputs)\n",
    "    labels = torch.stack(labels)\n",
    "    return inputs, labels\n",
    "\n",
    "# Advanced Audio Transform\n",
    "import torch\n",
    "import torchaudio\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "class AdvancedAudioTransform:\n",
    "    def __init__(self, sample_rate=16000, target_length=None):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.target_length = target_length\n",
    "    \n",
    "    def __call__(self, waveform):\n",
    "        try:\n",
    "            # Ensure waveform is a tensor\n",
    "            if not isinstance(waveform, torch.Tensor):\n",
    "                raise ValueError(\"Input must be a PyTorch tensor\")\n",
    "            \n",
    "            # Time stretch\n",
    "            if self.target_length is not None and torch.rand(1) < 0.5:\n",
    "                rate = 0.8 + torch.rand(1) * 0.4  # 0.8-1.2\n",
    "                waveform_np = waveform.numpy()\n",
    "                stretched = librosa.effects.time_stretch(waveform_np, rate=rate.item())\n",
    "                stretched = torch.from_numpy(stretched).float()\n",
    "                if stretched.shape[0] > self.target_length:\n",
    "                    start = torch.randint(0, stretched.shape[0] - self.target_length, (1,)).item()\n",
    "                    waveform = stretched[start:start + self.target_length]\n",
    "                else:\n",
    "                    padding = self.target_length - stretched.shape[0]\n",
    "                    waveform = torch.nn.functional.pad(stretched, (0, padding))\n",
    "            else:\n",
    "                waveform = waveform\n",
    "            \n",
    "            # Pitch shift\n",
    "            if torch.rand(1) < 0.5:\n",
    "                n_steps = -4 + torch.rand(1) * 8  # -4 to 4 steps\n",
    "                waveform = torchaudio.functional.pitch_shift(waveform, self.sample_rate, n_steps.item())\n",
    "            \n",
    "            # Background noise\n",
    "            if torch.rand(1) < 0.3:\n",
    "                noise = torch.randn_like(waveform) * 0.005\n",
    "                waveform = waveform + noise\n",
    "            \n",
    "            # Random gain\n",
    "            if torch.rand(1) < 0.3:\n",
    "                gain = 0.8 + torch.rand(1) * 0.4  # 0.8-1.2\n",
    "                waveform = waveform * gain\n",
    "            \n",
    "            return waveform\n",
    "        except Exception as e:\n",
    "            print(f\"Error in AdvancedAudioTransform: {e}\")\n",
    "            return waveform  # Return original waveform on failure to avoid None\n",
    "\n",
    "# Compute Class Weights\n",
    "def compute_class_weights(protocols_dir, audio_dir):\n",
    "    train_files = glob.glob(f\"{protocols_dir}/train_fold*.csv\")\n",
    "    labels = []\n",
    "    for fold_file in train_files:\n",
    "        annos = pd.read_csv(fold_file)\n",
    "        for idx in range(len(annos)):\n",
    "            clip_name = annos.iloc[idx, 0]\n",
    "            audio_path = os.path.join(audio_dir, f\"{clip_name}.wav\")\n",
    "            if os.path.exists(audio_path):\n",
    "                label = 0 if any(k in str(annos.iloc[idx, 1]).lower() for k in ['truth', '0']) else 1\n",
    "                labels.append(label)\n",
    "    labels = np.array(labels)\n",
    "    class_counts = np.bincount(labels)\n",
    "    total = len(labels)\n",
    "    weights = total / (2.0 * class_counts)\n",
    "    return torch.tensor(weights, dtype=torch.float)\n",
    "\n",
    "# Models from Updated Code\n",
    "class HTSAT(nn.Module):\n",
    "    \"\"\"Hierarchical Token-Semantic Audio Transformer (HTS-AT)\"\"\"\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.spec = T.MelSpectrogram(sample_rate=16000, n_fft=2048, hop_length=512, n_mels=128)\n",
    "        self.amplitude_to_db = T.AmplitudeToDB()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        \n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=128, nhead=8, dim_feedforward=512),\n",
    "            num_layers=6\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.spec(x)\n",
    "        x = self.amplitude_to_db(x)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.conv(x)\n",
    "        b, c, h, w = x.shape\n",
    "        x = x.permute(0, 2, 3, 1).reshape(b, h * w, c)\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "class S3D(nn.Module):\n",
    "    \"\"\"S3D Network with pretrained weights adapted for audio\"\"\"\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.s3d = torch.hub.load('facebookresearch/pytorchvideo', 's3d', pretrained=True)\n",
    "        # Modify for audio input (assuming waveform input)\n",
    "        self.s3d.blocks[0].conv = nn.Conv3d(1, 64, kernel_size=(5, 7, 7), stride=(1, 2, 2), padding=(2, 3, 3))\n",
    "        self.s3d.blocks[4].proj = nn.Linear(1024, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1).unsqueeze(-1).unsqueeze(-1)  # [B, 1, T, 1, 1]\n",
    "        return self.s3d(x)\n",
    "\n",
    "class AudioSpectrogramTransformer(nn.Module):\n",
    "    \"\"\"Modified AST with Learnable Positional Embeddings\"\"\"\n",
    "    def __init__(self, input_size=(128, 256), num_classes=2):\n",
    "        super().__init__()\n",
    "        self.patch_size = 16\n",
    "        self.hidden_size = 768\n",
    "        \n",
    "        self.conv = nn.Conv2d(1, self.hidden_size, kernel_size=self.patch_size, stride=self.patch_size)\n",
    "        \n",
    "        seq_len = (input_size[0] // self.patch_size) * (input_size[1] // self.patch_size)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, seq_len, self.hidden_size))\n",
    "        \n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=self.hidden_size, nhead=12, dim_feedforward=3072),\n",
    "            num_layers=12\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Linear(self.hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)  # [B, C, H, W]\n",
    "        x = x.flatten(2).permute(0, 2, 1)  # [B, N, C]\n",
    "        x += self.pos_embed\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "class WavegramCNN(nn.Module):\n",
    "    \"\"\"Waveform-based CNN with SincNet Filters\"\"\"\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.sincnet = nn.Sequential(\n",
    "            nn.Conv1d(1, 64, 251, stride=80, padding=125),\n",
    "            nn.MaxPool1d(4),\n",
    "            nn.Conv1d(64, 128, 5),\n",
    "            nn.MaxPool1d(4),\n",
    "            nn.Conv1d(128, 256, 3),\n",
    "            nn.MaxPool1d(4)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add channel dim\n",
    "        x = self.sincnet(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "class HybridModel(nn.Module):\n",
    "    \"\"\"Waveform + Spectrogram Hybrid Model\"\"\"\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.wave_model = WavegramCNN(num_classes=128)  # Intermediate features\n",
    "        self.spec_model = HTSAT(num_classes=128)        # Intermediate features\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        x_wave, x_spec = inputs\n",
    "        wave_feat = self.wave_model(x_wave)\n",
    "        spec_feat = self.spec_model(x_spec)\n",
    "        combined = torch.cat([wave_feat, spec_feat], dim=1)\n",
    "        return self.classifier(combined)\n",
    "\n",
    "# Mixup Function\n",
    "def mixup_data(x, y, alpha=1.0):\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "    if isinstance(x, tuple):\n",
    "        mixed_x = tuple(lam * xi + (1 - lam) * xi[index] for xi in x)\n",
    "    else:\n",
    "        mixed_x = lam * x + (1 - lam) * x[index]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "# Compute Metrics with EER\n",
    "def compute_metrics(preds, labels):\n",
    "    preds_np = preds.argmax(dim=1).cpu().numpy()\n",
    "    probs_np = preds[:, 1].cpu().numpy()\n",
    "    labels_np = labels.cpu().numpy()\n",
    "    \n",
    "    acc = accuracy_score(labels_np, preds_np)\n",
    "    f1 = f1_score(labels_np, preds_np)\n",
    "    recall = recall_score(labels_np, preds_np)\n",
    "    auc = roc_auc_score(labels_np, probs_np)\n",
    "    eer = compute_eer(labels_np, probs_np)\n",
    "    \n",
    "    return {\n",
    "        'loss': nn.CrossEntropyLoss()(preds, labels).item(),\n",
    "        'acc': acc,\n",
    "        'f1': f1,\n",
    "        'recall': recall,\n",
    "        'auc': auc,\n",
    "        'eer': eer\n",
    "    }\n",
    "\n",
    "def compute_eer(labels, probs):\n",
    "    fpr, tpr, _ = roc_curve(labels, probs)\n",
    "    fnr = 1 - tpr\n",
    "    min_index = np.nanargmin(np.abs(fnr - fpr))\n",
    "    eer = (fpr[min_index] + fnr[min_index]) / 2\n",
    "    return eer\n",
    "\n",
    "# Validation with TTA\n",
    "def validate_with_tta(model, loader, device, tta_steps=5, target_length=None):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    transform = AdvancedAudioTransform(sample_rate=16000, target_length=target_length)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            if isinstance(batch[0], tuple):\n",
    "                (waveform, spectrogram), labels = batch\n",
    "                waveform = waveform.to(device)\n",
    "                spectrogram = spectrogram.to(device)\n",
    "                inputs = (waveform, spectrogram)\n",
    "            else:\n",
    "                inputs, labels = batch\n",
    "                inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            preds = []\n",
    "            for _ in range(tta_steps):\n",
    "                if isinstance(inputs, tuple):\n",
    "                    # Augment only the waveform for HybridModel\n",
    "                    aug_wave = transform(inputs[0])\n",
    "                    aug_inputs = (aug_wave, inputs[1])  # Keep spectrogram unchanged\n",
    "                else:\n",
    "                    aug_inputs = transform(inputs)\n",
    "                \n",
    "                if aug_inputs is None:  # Safeguard against None\n",
    "                    aug_inputs = inputs\n",
    "                \n",
    "                outputs = model(aug_inputs)\n",
    "                preds.append(F.softmax(outputs, dim=1))\n",
    "            avg_preds = torch.stack(preds).mean(0)\n",
    "            all_preds.append(avg_preds)\n",
    "            all_labels.append(labels)\n",
    "    \n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    return compute_metrics(all_preds, all_labels)\n",
    "\n",
    "# Training Function\n",
    "def train_model(config):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using {device}\")\n",
    "    \n",
    "    # Compute class weights\n",
    "    class_weights = None\n",
    "    if config.get('class_weighting', False):\n",
    "        class_weights = compute_class_weights(config['protocols_dir'], config['audio_dir'])\n",
    "        class_weights = class_weights.to(device)\n",
    "        print(f\"Class weights: {class_weights}\")\n",
    "    \n",
    "    # Create checkpoint directory\n",
    "    os.makedirs(config['checkpoint_dir'], exist_ok=True)\n",
    "    metrics_file = os.path.join(config['checkpoint_dir'], 'metrics.csv')\n",
    "    if not os.path.exists(metrics_file):\n",
    "        pd.DataFrame(columns=['epoch', 'fold', 'train_loss', 'train_acc', 'train_f1', \n",
    "                              'train_recall', 'train_auc', 'train_eer', 'val_loss', \n",
    "                              'val_acc', 'val_f1', 'val_recall', 'val_auc', 'val_eer']).to_csv(metrics_file, index=False)\n",
    "    \n",
    "    # Get fold numbers\n",
    "    train_files = glob.glob(f\"{config['protocols_dir']}/train_fold*.csv\")\n",
    "    fold_numbers = sorted([f.split(\"train_fold\")[1].split(\".csv\")[0] for f in train_files])\n",
    "    \n",
    "    for fold in fold_numbers:\n",
    "        print(f\"\\n=== Training Fold {fold} ===\")\n",
    "        \n",
    "        # Initialize model\n",
    "        model = config['model']().to(device)\n",
    "        \n",
    "        # Optimizer and criterion\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=config['learning_rate'], \n",
    "                                weight_decay=config.get('weight_decay', 0.01))\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights, \n",
    "                                        label_smoothing=config.get('label_smoothing', 0.0))\n",
    "        \n",
    "        # Scheduler\n",
    "        scheduler = None\n",
    "        if config.get('scheduler') == 'cosine':\n",
    "            scheduler = CosineAnnealingLR(optimizer, T_max=config['num_epochs'])\n",
    "        elif config.get('scheduler') == 'cyclic':\n",
    "            scheduler = CyclicLR(optimizer, base_lr=1e-5, max_lr=1e-3, step_size_up=500)\n",
    "        \n",
    "        # EMA and SWA\n",
    "        ema_model = AveragedModel(model)\n",
    "        swa_model = AveragedModel(model)\n",
    "        swa_scheduler = SWALR(optimizer, swa_lr=config['learning_rate'] / 10)\n",
    "        \n",
    "        # Data loading\n",
    "        train_file = f\"{config['protocols_dir']}/train_fold{fold}.csv\"\n",
    "        val_file = f\"{config['protocols_dir']}/test_fold{fold}.csv\"\n",
    "        \n",
    "        transform = AdvancedAudioTransform(sample_rate=16000, target_length=config['target_length']) if config.get('augmentations', False) else None\n",
    "        train_dataset = AudioDataset(\n",
    "            train_file, config['audio_dir'], config['target_length'],\n",
    "            mode=config.get('mode', 'spectrogram'),\n",
    "            transform=transform,\n",
    "            spec_transform=None,\n",
    "            spec_params=config.get('spec_params')\n",
    "        )\n",
    "        val_dataset = AudioDataset(\n",
    "            val_file, config['audio_dir'], config['target_length'],\n",
    "            mode=config.get('mode', 'spectrogram'),\n",
    "            transform=None,\n",
    "            spec_transform=None,\n",
    "            spec_params=config.get('spec_params')\n",
    "        )\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], \n",
    "                                  collate_fn=audio_collate_fn, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], \n",
    "                                collate_fn=audio_collate_fn)\n",
    "        \n",
    "        best_val_auc = 0\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(config['num_epochs']):\n",
    "            model.train()\n",
    "            epoch_loss = 0\n",
    "            train_preds, train_labels = [], []\n",
    "            \n",
    "            for batch in train_loader:\n",
    "                if config.get('mode') == 'both':\n",
    "                    (waveform, spectrogram), labels = batch\n",
    "                    waveform = waveform.to(device)\n",
    "                    spectrogram = spectrogram.to(device)\n",
    "                    inputs = (waveform, spectrogram)\n",
    "                else:\n",
    "                    inputs, labels = batch\n",
    "                    inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # Mixup\n",
    "                if config.get('mixup_alpha', 0) > 0:\n",
    "                    inputs, labels_a, labels_b, lam = mixup_data(inputs, labels, config['mixup_alpha'])\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                if config.get('mixup_alpha', 0) > 0:\n",
    "                    loss = lam * criterion(outputs, labels_a) + (1 - lam) * criterion(outputs, labels_b)\n",
    "                else:\n",
    "                    loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Update EMA\n",
    "                ema_model.update_parameters(model)\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "                train_preds.append(outputs.detach())\n",
    "                train_labels.append(labels)\n",
    "            \n",
    "            # Update SWA\n",
    "            if epoch >= config.get('swa_start', 25):\n",
    "                swa_model.update_parameters(model)\n",
    "                swa_scheduler.step()\n",
    "            \n",
    "            # Validation with TTA\n",
    "            # Validation with TTA\n",
    "            val_metrics = validate_with_tta(ema_model, val_loader, device, \n",
    "                                            tta_steps=config.get('tta_steps', 5), \n",
    "                                            target_length=config['target_length'])\n",
    "            \n",
    "            # Compute train metrics\n",
    "            train_preds = torch.cat(train_preds)\n",
    "            train_labels = torch.cat(train_labels)\n",
    "            train_metrics = compute_metrics(train_preds, train_labels)\n",
    "            train_loss = epoch_loss / len(train_loader)\n",
    "            \n",
    "            # Save metrics\n",
    "            metrics = {\n",
    "                'epoch': epoch + 1,\n",
    "                'fold': fold,\n",
    "                'train_loss': train_loss,\n",
    "                **{f'train_{k}': v for k, v in train_metrics.items()},\n",
    "                **{f'val_{k}': v for k, v in val_metrics.items()}\n",
    "            }\n",
    "            pd.DataFrame([metrics]).to_csv(metrics_file, mode='a', header=False, index=False)\n",
    "            \n",
    "            # Early stopping and model saving\n",
    "            if val_metrics['auc'] > best_val_auc:\n",
    "                best_val_auc = val_metrics['auc']\n",
    "                patience_counter = 0\n",
    "                \n",
    "                # Save best EMA model\n",
    "                torch.save({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'model_state_dict': ema_model.module.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'val_auc': val_metrics['auc'],\n",
    "                    'config': config\n",
    "                }, os.path.join(config['checkpoint_dir'], f'best_ema_model_fold{fold}.pth'))\n",
    "                \n",
    "                print(f\"Fold {fold} Epoch {epoch+1}: New best model saved (val_auc: {val_metrics['auc']:.4f})\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if config.get('early_stopping', False) and patience_counter >= config['patience']:\n",
    "                    print(f\"Early stopping triggered for fold {fold} at epoch {epoch + 1}\")\n",
    "                    break\n",
    "            \n",
    "            if scheduler and not isinstance(scheduler, SWALR):\n",
    "                scheduler.step()\n",
    "            \n",
    "            print(f\"Fold {fold} Epoch {epoch+1}/{config['num_epochs']}: \"\n",
    "                  f\"Train Loss: {train_loss:.4f}, Val AUC: {val_metrics['auc']:.4f}\")\n",
    "        \n",
    "        # Save SWA model\n",
    "        torch.save({\n",
    "            'model_state_dict': swa_model.module.state_dict(),\n",
    "            'config': config\n",
    "        }, os.path.join(config['checkpoint_dir'], f'swa_model_fold{fold}.pth'))\n",
    "\n",
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Analyze dataset\n",
    "    dataset_stats = analyze_dataset(\n",
    "        protocols_dir='/home/azureuser/cloudfiles/code/Users/yashika22csu235/research/train_protocol',\n",
    "        audio_dir='/home/azureuser/cloudfiles/code/Users/yashika22csu235/research/audio_files'\n",
    "    )\n",
    "    \n",
    "    # Base configuration\n",
    "    base_config = {\n",
    "        'batch_size': 64,\n",
    "        'num_epochs': 100,\n",
    "        'learning_rate': 3e-4,\n",
    "        'audio_dir': '/home/azureuser/cloudfiles/code/Users/yashika22csu235/research/audio_files',\n",
    "        'protocols_dir': '/home/azureuser/cloudfiles/code/Users/yashika22csu235/research/train_protocol',\n",
    "        'base_checkpoint_dir': '/home/azureuser/cloudfiles/code/Users/yashika22csu235/research/train_new/experiment4/',\n",
    "        'target_length': int(dataset_stats['duration_stats']['median'] * 16000),\n",
    "        'augmentations': True,\n",
    "        'class_weighting': True if dataset_stats['class_counts'][0] != dataset_stats['class_counts'][1] else False,\n",
    "        'early_stopping': True,\n",
    "        'patience': 7,\n",
    "        'min_delta': 0.001,\n",
    "        'mixup_alpha': 0.4,\n",
    "        'label_smoothing': 0.1,\n",
    "        'scheduler': 'cosine',\n",
    "        'weight_decay': 0.05,\n",
    "        'swa_start': 25,\n",
    "        'tta_steps': 5,\n",
    "        'spec_params': {\n",
    "            'n_mels': 128,\n",
    "            'n_fft': 2048,\n",
    "            'hop_length': 512,\n",
    "            'f_min': 20,\n",
    "            'f_max': 8000\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Experiments\n",
    "    experiments = [\n",
    "       # {\n",
    "       #    'name': 'HTSAT_Advanced',\n",
    "        #   'model': HTSAT,\n",
    "         #  'config': {\n",
    "    #      'mode': 'waveform',  # HTSAT computes spectrogram internally\n",
    "          #      'learning_rate': 2e-4\n",
    "          #  }\n",
    "        #},\n",
    "        {\n",
    "            'name': 'S3D_Pretrained',\n",
    "            'model': S3D,\n",
    "            'config': {\n",
    "                'mode': 'waveform',\n",
    "                'learning_rate': 3e-5\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'name': 'AST_Deep',\n",
    "            'model': AudioSpectrogramTransformer,\n",
    "            'config': {\n",
    "                'mode': 'spectrogram',\n",
    "                'learning_rate': 5e-5\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'name': 'WavegramCNN',\n",
    "            'model': WavegramCNN,\n",
    "            'config': {\n",
    "                'mode': 'waveform',\n",
    "                'learning_rate': 1e-4\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'name': 'Hybrid_WaveSpec',\n",
    "            'model': HybridModel,\n",
    "            'config': {\n",
    "                'mode': 'both',\n",
    "                'learning_rate': 1e-4,\n",
    "                'scheduler': 'cyclic'\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for exp in experiments:\n",
    "        config = base_config.copy()\n",
    "        config.update(exp['config'])\n",
    "        config['model'] = exp['model']\n",
    "        config['checkpoint_dir'] = os.path.join(base_config['base_checkpoint_dir'], exp['name'])\n",
    "        print(f\"\\nRunning experiment: {exp['name']}\")\n",
    "        train_model(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
